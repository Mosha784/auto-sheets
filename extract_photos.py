import json
import gspread
import requests
from bs4 import BeautifulSoup
from oauth2client.service_account import ServiceAccountCredentials
import time
import re

# 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ø¬ÙˆØ¬Ù„ Ø´ÙŠØª
with open('service_account.json') as f:
    service_account_info = json.load(f)

scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
creds = ServiceAccountCredentials.from_json_keyfile_dict(service_account_info, scope)
client = gspread.authorize(creds)
sheet = client.open_by_url("https://docs.google.com/spreadsheets/d/1YFdOAR04ORhSbs38KfZPEdJQouX-bcH6exWjI06zvec/edit")
worksheet = sheet.worksheet("Missing In Form")

def clean_image_url(url):
    """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±Ø§Ø¨Ø· Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø©"""
    if not url: return None
    # Ø¥Ø²Ø§Ù„Ø© Ù„Ø§Ø­Ù‚Ø§Øª Ø§Ù„Ø­Ø¬Ù… Ù…Ø«Ù„ _300x300.jpg
    url = re.sub(r'_\d+x\d+.*$', '', url)
    if url.startswith('//'): url = "https:" + url
    return url

def get_image_statically(link):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø¯ÙˆÙ† Ù…ØªØµÙØ­ (Ø£Ø³Ø±Ø¹ ÙˆØ£Ø¯Ù‚ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø¨Ù„ÙˆÙƒ)"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept-Language": "en-US,en;q=0.9"
    }
    try:
        response = requests.get(link, headers=headers, timeout=15)
        if response.status_code != 200: return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 1. Ù…Ø­Ø§ÙˆÙ„Ø© og:image (Ø§Ù„Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©)
        meta_og = soup.find("meta", property="og:image")
        if meta_og and meta_og.get("content"):
            content = meta_og["content"]
            # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ù„ÙˆØ¬Ùˆ (Ø§Ù„Ù„ÙˆØ¬Ùˆ ØºØ§Ù„Ø¨Ø§Ù‹ PNG ÙˆØµÙˆØ±Ø© Ø§Ù„Ù…Ù†ØªØ¬ JPG)
            if ".jpg" in content.lower() and "tps-" not in content:
                return clean_image_url(content)
        
        # 2. Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø³ÙƒØ±ÙŠØ¨ØªØ§Øª Ø§Ù„ØµÙØ­Ø© Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµÙˆØ±
        scripts = soup.find_all("script")
        for script in scripts:
            if script.string and "imageConfig" in script.string:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙˆÙ„ Ø±Ø§Ø¨Ø· ØµÙˆØ±Ø© Ù…Ù†ØªØ¬ ÙŠÙ†ØªÙ‡ÙŠ Ø¨Ù€ .jpg
                match = re.search(r'(https:[^"]+\.jpg)', script.string)
                if match:
                    return clean_image_url(match.group(1).replace('\\u002F', '/'))
                    
    except Exception as e:
        print(f"Error fetching {link}: {e}")
    return None

# --- Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ ---
print("ğŸš€ Starting High-Speed Extraction Process...")
data = worksheet.get_all_values()

# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© (G Ù‡Ùˆ 7 Ùˆ H Ù‡Ùˆ 8)
COL_IMAGE = 7
COL_LINK = 8

for idx in range(1, len(data)):
    row = data[idx]
    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© ÙÙŠ Ø§Ù„ØµÙ
    img_val = row[COL_IMAGE-1] if len(row) >= COL_IMAGE else ''
    link = row[COL_LINK-1] if len(row) >= COL_LINK else ''
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø®Ù„ÙŠØ© Ø§Ù„ØµÙˆØ±Ø© ÙØ§Ø±ØºØ© ÙˆÙ‡Ù†Ø§Ùƒ Ø±Ø§Ø¨Ø· Ù…Ù†ØªØ¬
    if (not img_val or not img_val.strip()) and link and link.strip():
        print(f"ğŸŒ Row {idx+1}: Processing {link[:50]}...")
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³Ø±ÙŠØ¹
        img_url = get_image_statically(link)
        
        if img_url:
            worksheet.update_cell(idx+1, COL_IMAGE, img_url)
            print(f"âœ… Success: {img_url}")
            # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ· Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø± Ù…Ù† Ø¬ÙˆØ¬Ù„ Ø´ÙŠØª
            time.sleep(1)
        else:
            print(f"âŒ Failed to find product image for row {idx+1}")

print("ğŸ‰ Task Completed Successfully.")
